# ðŸ§­ LLM Gateway Execution Flow

This document describes the complete execution flow of the LLM Gateway. Each stage is governed by persona-specific configurations and is modularly handled by dedicated components.

---

## ðŸ” End-to-End Flow

```
User Request
    â†“
Load Persona Properties
    â†“
Fetch Chat History (if enabled)
    â†“
Query Rewrite (if enabled)
    â””â”€â”€> Fetch relevant prompt based on user query
          â†“
      Model Routing Component â†’ LLM Call (for query rewrite)
    â†“
RAG (if enabled in persona config)
    â”œâ”€â”€ Vector Search
    â”‚   â”œâ”€â”€ Convert context text into segments
    â”‚   â”œâ”€â”€ Generate embeddings (LLM Call via Model Router)
    â”‚   â””â”€â”€ Query vector DB (e.g., Weaviate) using generated vector + persona filter
    â”œâ”€â”€ Web Search
    â”‚   â”œâ”€â”€ Trigger web search with user query
    â”‚   â”œâ”€â”€ Load web search config (HTML selectors etc.) from persona
    â”‚   â””â”€â”€ Parse HTML using config to extract relevant content
    â””â”€â”€ Combine results if both are enabled
    â†“
Prepare Gateway Request (Main POJO with all runtime context)
    â†“
Final LLM Call via Model Routing Component
    â”œâ”€â”€ Fetch model-specific properties
    â”œâ”€â”€ Select prompt (based on rewritten query and persona phase)
    â”œâ”€â”€ Load tools (if enabled in persona)
    â””â”€â”€ Construct LLM request and make API call
    â†“
Tool Calling Phase (if model requests tools)
    â””â”€â”€> Tool Resolver
          â”œâ”€â”€ Resolve endpoint from tool name
          â”œâ”€â”€ Use HTTP connection pooling, retries, auth handlers
          â””â”€â”€ Call external MCP server
    â†“
Final Response Returned to User
```

---

## ðŸ’¡ Notes

* **Persona Properties** drive core behaviors: enabling/disabling chat history, query rewrite, RAG, and tool usage.
* **Model Routing Component** is invoked multiple times for: query rewrite LLM call, vector embedding, and final LLM inference.
* **Gateway Request** is the central runtime object encapsulating everything â€” query details, prompt, tools, results, etc.
* **Tool Calls** are dynamic and handled only if the LLM requests tool execution. These calls use robust HTTP management (retry logic, pooling, authentication).

---

<h3>ðŸ”„ Flow Highlights by Stage</h3>

<div style="overflow-x:auto">
  <table>
    <thead>
      <tr>
        <th style="text-align:left;">Stage</th>
        <th style="text-align:left;">Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Query Rewrite</td>
        <td>Transforms user query contextually via relevant prompt and LLM inference.</td>
      </tr>
      <tr>
        <td>RAG</td>
        <td>Augments context using vector DB, web search, or both.</td>
      </tr>
      <tr>
        <td>Prompt Selection</td>
        <td>Chooses the best prompt for the current persona and phase.</td>
      </tr>
      <tr>
        <td>Tool Execution</td>
        <td>Executes tools if invoked by the model using MCP service integration.</td>
      </tr>
      <tr>
        <td>Model Routing</td>
        <td>Centralized abstraction to route to the correct LLM and config.</td>
      </tr>
    </tbody>
  </table>
</div>


---

This document serves as a foundational overview for developers and contributors working on or integrating with the LLM Gateway.